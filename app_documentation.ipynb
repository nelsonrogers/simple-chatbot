{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif\n",
    "\n",
    "Créer un pipeline complet pour déployer un chatbot basé sur un modèle de langage (LLM) avec un suivi automatisé des performance.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Préparation des Données :\n",
    "\n",
    "    - Utiliser un modèle de langage pré-entraîné tel que (Phi3, Mistral7B, Llama3, …) disponible via la bibliothèque transformers de Hugging Face.\n",
    "    \n",
    "2. Développement du Chatbot :\n",
    "\n",
    "    - Implémenter un chatbot simple utilisant le modèle de langage choisi.\n",
    "    - Utiliser streamlit pour créer une interface utilisateur interactive pour le chatbot.\n",
    "\n",
    "3. Déploiement du Chatbot :\n",
    "\n",
    "    - Déployer l'application streamlit gratuitement en utilisant la plateforme Streamlit Share.\n",
    "    - Fournir un lien vers l'application déployée.\n",
    "\n",
    "4. Surveillance du Chatbot :\n",
    "\n",
    "    - Implémenter un mécanisme de surveillance pour suivre les interactions du chatbot. Par exemple, enregistrer les questions posées par les utilisateurs et les réponses fournies par le chatbot.\n",
    "    - Utiliser SQLite (une base de données intégrée et gratuite) pour stocker les logs de performance.\n",
    "\n",
    "### Livrables\n",
    "\n",
    "- Un Jupyter Notebook contenant :\n",
    "    - Le code source pour le développement et le déploiement du chatbot.\n",
    "    - La documentation détaillée de chaque étape.\n",
    "- Un lien vers l'application streamlit déployée : **https://peaks-chatbot-nelson.streamlit.app/**\n",
    "\n",
    "\n",
    "\n",
    "**Le code reste du test est rédigé exclusivement en anglais.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install & import the required libraries\n",
    "\n",
    "It's important to provide specific versions in order to ensure compatibility over time. These libraries should be included in the requirements.txt file for deployment (see deployment section) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.41.2 torch==2.3.1 streamlit==1.35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import sqlite3\n",
    "from transformers import pipeline, Conversation, AutoTokenizer\n",
    "from uuid import uuid4\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data: download model from Hugging Face\n",
    "\n",
    "Since model quality is not the objective, I chose a small model so as to limit download time and potentially inference time as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "Caching the loading of the model allows the app to run more efficiently as it doesn't have to load the model for each request.\n",
    "\n",
    "In this case, we chose a model capable of conversational tasks which allows it to have back and forth communication with the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "@st.cache_resource # load the model only once\n",
    "def load_model():\n",
    "    print(\"Loading model...\")\n",
    "    return pipeline(task=\"conversational\", model=\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "chatbot = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenizer\n",
    "\n",
    "Caching the loading of the tokenizer allows the app to run more efficiently as it doesn't have to load the tokenizer for each request.\n",
    "\n",
    "Note that we apply a template to the tokenizer - this is not required but it makes the chat template explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "@st.cache_resource # load the tokenizer only once\n",
    "def load_tokenizer():\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "    ]\n",
    "    tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up SQLite database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to SQLite database (or create it if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('chat_history.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty table to store chat history\n",
    "\n",
    "A timestamp is included for additional information. It also allows for chronological sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SQLite table...\n"
     ]
    }
   ],
   "source": [
    "@st.cache_resource # create the table only once\n",
    "def create_table():\n",
    "    print(\"Creating SQLite table...\")\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS chat_history (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            role TEXT NOT NULL,\n",
    "            content TEXT NOT NULL,\n",
    "            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: interact with the database\n",
    "\n",
    "Read and write functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get chat history from the database\n",
    "def get_chat_history():\n",
    "    c.execute(\"SELECT role, content FROM chat_history ORDER BY timestamp ASC\")\n",
    "    return c.fetchall()\n",
    "\n",
    "# Function to add a new message to the chat history\n",
    "def add_to_chat_history(role, content):\n",
    "    c.execute(\"INSERT INTO chat_history (role, content, timestamp) VALUES (?, ?, ?)\", (role, content, datetime.now()))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App functionality\n",
    "\n",
    "### Basic display\n",
    "\n",
    "The app (**Nelson's Simple Chatbot**) displays an text submission form that will return an answer from the language model. \n",
    "\n",
    "The chat history (prompts & responses) will be displayed above the form in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Nelson's Simple Chatbot\")\n",
    "\n",
    "# Display chat history\n",
    "chat_history = get_chat_history()\n",
    "for role, content in chat_history:\n",
    "    st.write(f\"{role.capitalize()}: {content}\")\n",
    "\n",
    "# Input area for new messages\n",
    "with st.form(key='chat_form', clear_on_submit=True):\n",
    "    user_input = st.text_input(\"You: \", \"\")\n",
    "    submit_button = st.form_submit_button(\"Submit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing user input\n",
    "When the user submits a prompt, it gets added to chat history and a response in generated by the language model.\n",
    "\n",
    "In order to preserve the language model's memory, we retrieve all chat history as input for the response.\n",
    "\n",
    "The tokenizer is then used to check that the model's token limit (128) has not been exceeded, before generating the response and updating the display. If the token limit has been reached, the user is asked to start a new chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process user input\n",
    "if submit_button and user_input:\n",
    "\n",
    "    add_to_chat_history('user', user_input)\n",
    "\n",
    "    full_chat = [{\"role\": role, \"content\": content} for role, content in get_chat_history()]\n",
    "\n",
    "    # Process full chat history into single string to count tokens\n",
    "    chat_input = \" \".join([f\"{message['role']}: {message['content']}\" for message in full_chat])\n",
    "\n",
    "    # Check if the token limit is exceeded\n",
    "    if len(tokenizer.tokenize(chat_input)) < tokenizer.model_max_length:\n",
    "        \n",
    "        conversation = Conversation(messages=full_chat)\n",
    "        conversation = chatbot(conversation)\n",
    "        \n",
    "        response = conversation[-1]['content'].strip()\n",
    "        add_to_chat_history('assistant', response)\n",
    "\n",
    "        # Refresh the app to display the new chat\n",
    "        st.rerun()\n",
    "    \n",
    "    else:\n",
    "        st.write(\"The token limit is exceeded. Please start a new chat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting a new conversation\n",
    "If at any point you wish to start a new conversation, you may click \"Start New Chat\".\n",
    "This will delete chat history in order to start over.\n",
    "\n",
    "Of course, in a real scenario, we would likely not delete the chat history but rather archive it for any future use. In this case, since we will not be using the data, we simply delete it.\n",
    "\n",
    "The app display is then updated using `st.rerun()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if st.button(\"Start New Chat\"):\n",
    "    c.execute(\"DELETE FROM chat_history\")\n",
    "    conn.commit()\n",
    "    st.rerun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the app\n",
    "\n",
    "In order to run the app, you must convert this notebook into a `.py` file and run the following command: `streamlit run app.py` where app.py is the name of your file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the app\n",
    "\n",
    "In order to deploy the app to Streamlit Share, you must upload the project to GitHub. Your project directory should look something like this:\n",
    "```\n",
    "your-repository/\n",
    "├── your_app.py\n",
    "└── requirements.txt\n",
    "```\n",
    "\n",
    "or like this (if you include any custom configurations):\n",
    "\n",
    "```\n",
    "your-repository/\n",
    "├── .streamlit/\n",
    "│   └── config.toml\n",
    "├── your_app.py\n",
    "└── requirements.txt\n",
    "```\n",
    "\n",
    "You can then deploy the app directly from the streamlit website based on your GitHub repository: https://share.streamlit.io/new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
